# 深度学习

## day1 深度学习概论

### 深度学习

深度学习 即 训练神经网络

​	我们从一些房屋的面积和它们的价格，找到一个函数根据面积预测房价。线性回归找到个直线，很容易解决这个问题。完善一下就是个简单的神经网络。这个神经网络的输入```size``` 经过 ```neuro``` 输出 ```price```。

​	根据房屋其他属性，房间数、环境、学区房等人们关注的问题都会与房价有关，这些都可以作为神经网络的输入。利用训练集所有中间过程都会自动完成，我们实际上要做的是给出训练集。

![1](1.png)

### 监督学习 Supervised Learning

 *  标准神经网络 standard neural network 

    ​	房地产、广告

 *  卷积神经网络 CNN (convolutional neural network)

    ​	图形领域

* 循环神经网络 RNN (recurrent neural network)

  ​	序列 语音识别

* 复杂循环神经网络 RNNS

  ​	更复杂的序列 语言，英语和汉语，字母或单词

* 混合神经网络结构

  ​	无人驾驶

  ![1](3.png)

**结构化数据**类似于数据库的结构

**非结构化数据**例如音频、图像、文章等



深度学习的兴起与数据规模和神经网络的规模有关

![1](4.png)



![1](5.png)

不停地迭代训练神经网络，计算上优秀的算法会有很大帮助

## day2 神经网络基础(1)

### logistic回归

​	logistics回归是一个用于二分分类的算法，例如用来区分一个图片中是否有猫。

​	计算机保存一张图片，要保存三个独立矩阵，分别对应图片中的 ```Red``` ```Green``` ```Blue``` 三个颜色通道。如果输入图片是*64\*64*像素的，就有三个*64\*64*的矩阵，分别对应三种像素的亮度。

![1](21.png)

​	为了计算的便捷，通常把图片中像素矩阵表示成向量形式。如果是64\*64的矩阵，那么表示输入的特征向量```x``` 的维度 $n_x = 64*64*3=12288$ （$n_x$简写为$n$）。在二分分类问题中，目标是训练出一个分类器，它以图片的特征向量$x$作为输入，预测输出的结果标签$y$是```1``` or ```0```，即是否有猫。

![1](22.png)

​	Given $x$, output $\hat{y} = P(y =1|x)$,where $ 0 \le \hat{y} \le 1 $ 

​	问是猫的概率。

​	解：

​	$ \hat{y} = \sigma(w^Tx+b) $ 

​			$w$ is weights , $w \in \mathbb{R}^{n_x}$ 

​			$b$ is threshold, $b \in \mathbb{R}$ 

#### Sigmoid Function

​	$s = \sigma(w^Tx+b) = \sigma(z) = \frac {1}{1+e^{-z}} $

​			![1](23.png)

​	为了训练参数 $w$ 和 $b$ , 我们需要定义一个```cost function``` 。

#### Loss Function

损失函数（误差函数） 定义在单个训练样本上的，也就是就算一个样本的误差。比如我们想要分类，就是预测的类别和实际类别的区别，是一个样本的哦，用**L**表示

$L(\hat{y}^{(i)},{y}^{(i)}) = \frac {1}{2}(\hat{y}^{(i)}-{y}^{(i)})^2$ 

在logistic回归中用下面的 Loss function 更合适

$L(\hat{y}^{(i)}, y^{(i)}) = -(y^{(i)}log(\hat{y}^{(i)}) +(1 - {y}^{(i)})log(1-\hat{y}^{(i)}))$ 

#### Cost Function

代价函数 (成本函数) 定义在整个训练集上面的，也就是所有样本的误差的总和的平均，用 **J** 表示

我们要找到合适的参数$w$ 和 $b$ 最小化代价函数

$ J(w, b) = \frac{1}{m}\sum_{i=1}^{m}L(\hat{y}^{(i)}, y^{(i)})$

### 符号约定

​	$(x,y)$ 表示一个单独的样本

​	$x \in \mathbb {R}^{n_x} $ $x$是$n_x$维的特征向量

​	$y \in \{0,1\}$ 标签$y$值为```0```或```1```

​	m training examples : $\{(x^{(1)},y^{(1)}),(x^{(2)},y^{(2)}),...(x^{(m)},y^{(m)})\}$ 训练集

​	$m$ 表示训练样本的个数

​	$ m_{train}$  训练集样本数 	 $m_{test}$ 测试集样本数

​	$X =\begin{bmatrix}\vdots      & \vdots & \vdots & \vdots     \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots      & \vdots & \vdots & \vdots  \end{bmatrix}$ 表示所有训练样本的输入

​	$X \in \mathbb {R}^{n_x * m}$  	

​	$Y=\begin{bmatrix} y^{(1)} & y^{(2)} &  \cdots  & y^{(m)} \end{bmatrix}$ 表示所有训练样本的输出

​	$ Y \in \mathbb{R}^{1*m}$ 

​	```	X.shape``` python命令输出$X$的维度

## day3 神经网络基础(2)

### 梯度下降法

​	为了习得合适的参数$w$ 和 $b$ ，我们要使 $ J(w, b)$ 尽可能小。

![1](31.png)

在上图 **凸函数** 中找到最小值的过程：

1. 初始化 $w$ 和 $b$ (任意值，通常为$0$)

2. 朝着最快下降的方向下降 

   $w := w - \alpha \frac {\partial J(w,b)}{\partial w}$

   $b := b - \alpha \frac {\partial J(w,b)}{\partial b}$

   $\alpha$为学习率，可以控制每次迭代或梯度下降法中的步长

3. $J(w,b)$ 为最小值时结束，否则重复第二步

### 计算图

在梯度下降法中要计算$\frac {\partial J(w,b)}{\partial w}$，直接计算很不容易，我们借助**链式法则**和**计算图**可以很方便的求出。

例：$J(a,b,c) = 3(a + bc) = 3(5+3*2)=33$

$u = bc$

$v = a+u$

$J = 3v$

![1](32.png)

**链式法则** $\frac {\partial J}{\partial b} = \frac {\partial J}{\partial v} \frac {\partial v}{\partial u}\frac {\partial u}{\partial b}$ 

**计算图**可以避免重复快速求出$\frac {\partial J}{\partial a}$ $\frac {\partial J}{\partial b}$ $\frac {\partial J}{\partial c}$ $\frac {\partial J}{\partial u}$  $\frac {\partial J}{\partial v}$ 

### 向量化

logistic回归的梯度下降法

![1](33.png)

尽量使用向量化来消除for循环，向量可以并行处理数据，而for循环不可以

例：计算 $z = w^Tx +b $ 	$w \in \mathbb{R}^{n_x}$		$x \in \mathbb{R}^{n_x}$ 

Non-Vectorization

```python
z = 0
for i in range(n_x):
	z += w[i]*x[i]
z += b
```

Vectorization

```python
z = np.dot(w, x) + b
```

在时间效率上，向量化可以节省很多时间

尽量规避 for-loops ，能不用就不用



更多的向量化例子:

```pyhton
import numpy as np
u = np.exp(v)
np.log(v)
np.abs(v)
np.maximum(v,0)
v**2
```

$$
z^{(i)} = w^T x^{(i)} + b \\ \hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)}) = sigmoid( w^T x + b) = \frac{1}{1 + e^{-(w^T x + b)}} \\ \mathcal{L}(a^{(i)}, y^{(i)}) = - y^{(i)} \log(a^{(i)}) - (1-y^{(i)} ) \log(1-a^{(i)}) \\ \frac{\partial J}{\partial w} = \frac{1}{m}X(A-Y)^T \\ \frac{\partial J}{\partial b} = \frac{1}{m} \sum_{i=1}^m (a^{(i)}-y^{(i)})
$$

#### 梯度下降的一次迭代

$J = 0, dw1 = 0, dw2 = 0, db = 0$
$for\ i = 1 \ to \ m:$
​	$z^{(i)} = w^Tx^{(i)}+b$

​	$a^{(i)} = \sigma (z^{(i)})$

​	$J += -[y^{(i)}log\hat{y}^{(i)} + (1-y^{(i)})log(1-\hat{y}^{(i)})]$

​	$d𝑧^{(i)} = 𝑎^{(i)} −𝑦^{(i)}$

​	$ d𝑤1 += 𝑥1 ^{(i)}d𝑧^{(i)} $

​	$d𝑤2 += 𝑥2 ^{(i)}d𝑧^{(i)} $

​	$db += d𝑧^{(i)} $

$J = J/m, d𝑤1 = d𝑤1/m, d𝑤2 = d𝑤2/m ,db = db/m $ 

上面代码改成向量化：

```python
Z = np.dot(w.T, X) + b
A = 1/(1+np.exp(-Z)) 	#sigmoid function
dZ = A - Y
dw = 1/m * dZ.T
db = 1/m * np.sum(dZ)
w = w - alpha * dw
b = b - alpha * db
```

## day4 神经网络基础(3)

### python基础

![1](41.png)

### logistic回归python实现

![1](42.png)

## day5 浅层神经网络(1)

### logistic回归模型

![1](53.png)

在这里你需要输入特征$x$，参数$w$和$b$, 计算出 $a = \hat{y} = \sigma(z)$ ,接下来可以计算损失函数 $L$ 。 

### 神经网络

神经网络类似logistic回归模型，就是把很多sigmoid单元堆叠起来构成一个神经网络

只有一个隐藏层的神经网络如下图：

![1](54.png)

输入层 $X$ 记做 $a^{[0]}$， 中间层为隐藏层(Hidden Layer) 记做 $a^{[1]}$，输出层记做 $a^{[2]}$ 。

与**logistic回归**相同每层的输出为 $a = \sigma(z)​$ ，中间层的值为上层的输出值。

**logistic回归**中的反向传播计算导数的过程，在神经网络中也适用。

#### 神经网络表示

![1](51.png)

$a^{[1]}_i$ 的计算都是一个sigmoid函数计算。因为每个$a^{[1]}_i$的计算都需要 $w\ b$，为了避免使用 ```for-loops``` ，需要将这层计算用到的 $w\ b$ 分别堆叠起来成为矩阵。

上图的神经网络的单一计算过程：

​	Given input $x$： 		$x \in \mathbb{R^{3*1}}$

​	$z^{[1]} = W^{[1]}x + b^{[1]}$ 	$W^{[1]} \in \mathbb{R}^{4*3}, b^{[1]} \in \mathbb{R}^{4*1}, z \in \mathbb{R}^{4*1} $
​	$a^{[1]} = \sigma(z^{[1]}) $			$a^{[1]} \in \mathbb{R}^{4*1}$
​	$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]} $	$W^{[2]} \in \mathbb{R}^{1*4}, b^{[2]} \in \mathbb{R}^{1*1}, z^{[2]} \in \mathbb{R}^{1*1} $
​	$a^{[2]} = \sigma(z^{[2]})$			$a^{[2]} \in \mathbb{R}^{1*1}$

$W$ 就是 $w^T$ 的堆叠矩阵。

### 神经网络计算的向量化

如果要计算多个例子，最直观的方法是在最上面加个```for-loops```。

for i = 1 to m:

​	$z^{[1](i)} = W^{[1](i)}x + b^{[1](i)}$ 
​	$a^{[1](i)} = \sigma(z^{[1](i)}) $
​	$z^{[2](i)} = W^{[2](i)}a^{[1](i)} + b^{[2](i)} $
​	$a^{[2](i)} = \sigma(z^{[2](i)})$

去掉```for-loops```，向量化实现:

$X =\begin{bmatrix}\vdots      & \vdots & \vdots & \vdots     \\ x^{(1)} & x^{(2)} & \cdots & x^{(m)}\\\vdots      & \vdots & \vdots & \vdots  \end{bmatrix}$	$x \in \mathbb{R}^{n_x*m}$ 

$Z^{[1]} = W^{[1]}X + b^{[1]}$ 	
$A^{[1]} = \sigma(Z^{[1]}) $	
$Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]} $
$A^{[2]} = \sigma(Z^{[2]})$	

![1](55.png)

## day6 浅层神经网络(2)

创建神经网络时，你可以选择隐藏层里用哪一个激活函数。在以上的例子的都是用的```sigmoid function```，有时其它的函数会更好。

![1](61.png)

其中，```ReLU function```已是现在的默认选择，有时也会选择```tanh function```。 

```ReLU function```的缺陷是当$z<0$时，导数为$0$。如果这个缺陷对你的问题有影响，可以使用```leaky ReLU function```。 ```ReLU function```和```leaky ReLU function```中$z$空间的导数(激活函数的斜率)和$0$差很远，在实际使用中，你的神经网络的学习速度通常会快的多。因为导数接近$0$时会减慢学习效率。

### 激活函数的选择

#### sigmoid function

$$
a = \frac {1}{1+e^{-z}}
$$

除非在二元分类的输出层，不然绝对不要用。

#### tanh function

$$
a = \frac {e^z -e^{-z}} {e^z +e^{-z}}
$$

在几乎所有场合都比```sigmoid function```优越。

#### ReLU function

$$
a = max(0, z)
$$

默认激活函数。不确定用哪个时，就用这个。

#### leaky ReLU function

$$
a = max(0.01z, z)
$$

你想用的话，可以试试这个。



为什么需要非线性激活函数？

因为使用线性的激活函数，无论多少层，最后只是一层的效果。



### 激活函数的导数

#### Sigmoid activation function

$$
a = g(z) = \frac {1}{1+e^{-z}} \\
g'(z) = \frac {1}{1+e^{-z}}(1-\frac {1}{1+e^{-z}}) = a(1-a)
$$

#### Tanh activation function

$$
a = g(z) = tanh(z) = \frac {e^z -e^{-z}} {e^z +e^{-z}} \\
g'(z) = 1-(tanh(z))^2 = 1 -a^2
$$

#### ReLU activation function

$$
a  = g(z) = max(0, z) \\
g'(z) = \begin{cases}
0, z < 0 \\
1,  z \ge 0 \end{cases}
$$

#### Leaky ReLU activation function

$$
a  = g(z) = max(0.01z, z) \\
g'(z) = \begin{cases}
0.01, z < 0 \\
1,  z \ge 0 \end{cases}
$$

### 神经网络的梯度下降

$$
已知参数parameters: W^{[1]},b^{[1]},W^{[2]},b^{[2]} ，n_x = n^{[0]},n^{[1]},n^{[2]}\\
W^{[1]}\in \mathbb{R}^{n^{[1]}*n^{[0]}} \\
b^{[1]}\in \mathbb{R}^{n^{[1]}*1} \\
W^{[2]}\in \mathbb{R}^{n^{[2]}*n^{[1]}} \\
b^{[2]}\in \mathbb{R}^{n^{[2]}*1} \\
成本函数cost\ function: J( W^{[1]},b^{[1]},W^{[2]},b^{[2]}) = 1/m \sum_{i=1}^n L(\hat y, y) \\
梯度下降Gradient\ descent: \\
Repeat\{	\\
计算预测值Compute\ predictions\ (\hat y ^{(i)}, i = 1 ... m)\\
dw^{[1]} = \frac {\sigma J}{\sigma w^{[1]}}, db^{[1]} = \frac {\sigma J}{\sigma b^{[1]}} ,... \\
    W^{[1]} = W^{[1]} - \alpha dw^{[1]} \\
    b^{[1]} = b^{[1]} - \alpha db^{[1]} \\
    W^{[2]} := ... \\
    b^{[2]} := ... \\
\\ \}
$$

#### 正向传播和反向传播的计算过程

![1](62.png)

### 随机初始化

随机初始化权重非常重要。

对于logistic回归，可以将权重初始化为零。但如果将神经网络的各参数数组全部初始化为零，再使用梯度下降算法，那会完全失效。在使用```sigmoid or tanh function```时，通常使$W$随机初始化并乘以$0.01$，确保激活函数不会接近饱和，从而减慢学习速度。 

## day7 深度神经网络

### 深层网络中的前向传播

![1](71.png)

$a^{[0]} = x$

$z^{[1]} = W^{[1]}a^{[0]}+b^{[1]}$

$a^{[1]} = g^{[1]}(z^{[1]})$

$z^{[2]} = W^{[2]}a^{[1]}+b^{[2]}$

$a^{[2]} = g^{[2]}(z^{[2]})$

...

$z^{[4]} = W^{[4]}a^{3]}+b^{[4]}$

$a^{[4]} = g^{[4]}(z^{[4]})$

$\hat y = a^{[4]}$



可以整理为

for i in 1 to m:

​	$Z^{[i]} = W^{[i]}A^{[i-1]} + b^{[i]}$

​	$A^{[i]} = g^{[i]}(Z^{[i]})$

$\hat Y = A^{[m]}$

在这里使用```for-loops```是可以的。

### 核对矩阵的维数

在神经网络的实现过程中，想增加得到没有bug的程序的概率...其中的一个方法就是需要非常仔细和系统化地去思考矩阵的维数。



